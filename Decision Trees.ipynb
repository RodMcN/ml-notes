{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees and forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/flow_charts.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees can be used in both classification and regression problems and are a class of supervised learning techniques. Thet operate by recursively dividing the feature space into adjacent cubiod regions by learning a hierarchy of simple binary models each working on a single variable in a tree structure. They are relatively simple models which work well on both numerical and categorical variables and are strongly favoured for their interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itâ€™s important to note that each split is based on a single variable. If boundaries more complicated than a simple inequality are used, it becomes unfeasibly difficult to generate trees at all. Still, given the combinatorial complexity, trees are optimised by greedy means, starting with a single root node, corresponding to the whole input space, and then growing the tree by adding nodes one at a time incorporating empirical heuristics. Decision trees are built by recursively adding nodes to a binary tree, with leaf nodes corresponding to output variables or classes, to minimise a loss function such as SSE or MSE. In practice, it would be relatively trivial to minimise such as loss function by splitting the input space into N subregions for an N sized training set, with each leaf of the tree containing a single training point. Therefore, it is important to consider when to stop adding new nodes. It is common practice to grow a large tree, using a stopping criterion based on the number\n",
    "of data points associated with the leaf nodes, and then prune back the resulting tree based on redundent or minimally informative nodes.\n",
    "\n",
    "There are 2 main algorithms commonly used in a variety of domains which are effective in constructing both decision and classification trees. These are CART (Classification And Regression Tree) and C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tree is grown starting from the root node by repeatedly using the following steps on each leaf node.\n",
    "1. Find the best split for each variable\n",
    "2. Among the best splits found in step 1, choose the one that maximises or minimises the splitting criterion.\n",
    "3. Split the node using its best split found in step 2 if the stopping rules are not satisfied.\n",
    "\n",
    "\n",
    "### Stopping criteria:\n",
    "Nodes will not be split if:\n",
    "* If all examples in a node have identical values/class.\n",
    "* If all examples in a node have identical values for each predictor variable.\n",
    "* If the size of a node is less than a predefined minimum node size.\n",
    "* If the split of a node results in a child node whose size is less than the specified minimum.\n",
    "* If the depth of the node reaches a predefined limit.\n",
    "* If the improvement in the loss metric is below some threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splites are made based on information gain. The most popular metrics for information gain are entropy and the Gini impurity measure.\n",
    "\n",
    "If there are C total classes and p(i) is the probability of picking a datapoint with class i, then the Gini Impurity is calculated as:\n",
    "$$\\sum^C_{i=1}p(i)(1-p(i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(classes):\n",
    "    assert isinstance(classes, (np.ndarray, list))\n",
    "    if type(classes) == list:\n",
    "        classes = np.array(classes)\n",
    "        \n",
    "    impurity = 1\n",
    "    for i in set(classes):\n",
    "        p = np.mean(classes == i)\n",
    "        impurity -= p**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity2(classes):\n",
    "    assert isinstance(classes, (np.ndarray, list))\n",
    "    if type(classes) == list:\n",
    "        classes = np.array(classes)\n",
    "        \n",
    "    impurity = 0\n",
    "    for i in set(classes):\n",
    "        p = np.mean(classes == i)\n",
    "        impurity += p*(1-p)\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, col, val, classes=None):\n",
    "    idx = data[..., col] <= val\n",
    "    l = data[idx]\n",
    "    r = data[~idx]\n",
    "    if classes is not None:\n",
    "        lc = classes[idx]\n",
    "        rc = classes[~idx]\n",
    "        return (l, lc), (r, rc)\n",
    "    return l, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.leaf = True\n",
    "        \n",
    "        self.split_column = None\n",
    "        self.split_value = None\n",
    "        self.value = None\n",
    "        \n",
    "        self.l_branch = None\n",
    "        self.r_branch = None\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        if self.leaf:\n",
    "            return self.value\n",
    "        elif data[self.split_column] <= self.split_value:\n",
    "            return self.l_branch(data)\n",
    "        else:\n",
    "            return self.r_branch(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(inputs, targets, root=None):\n",
    "    \n",
    "    if root is None:\n",
    "        root = Node()\n",
    "    \n",
    "    #evaluate splits\n",
    "    minpurity = 1\n",
    "    mc = mv = -1\n",
    "    for col in range(inputs.shape[-1]):\n",
    "        for val in set(inputs[:, col]):\n",
    "            (lsplit, lclasses), (rsplit, rclasses) = split(inputs, col, val, targets)\n",
    "            impurity = gini_impurity(lclasses) + gini_impurity(rclasses)\n",
    "            if impurity < minpurity:\n",
    "                minpurity = impurity\n",
    "                mc = col\n",
    "                mv = val\n",
    "    if len(lclasses) == 0 or len(rclasses) == 0 or mc == -1:\n",
    "        return\n",
    "    \n",
    "    root.split_column = mc\n",
    "    root.split_value = mv\n",
    "    root.leaf = False\n",
    "        \n",
    "    root.l_branch = Node()\n",
    "    build_tree(lsplit, lclasses, root.l_branch)\n",
    "    \n",
    "    root.r_branch = Node()\n",
    "    build_tree(rsplit, rclasses, root.r_branch)\n",
    "    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, we used the only change needed is to the to loss function. Usually the squared-error is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
